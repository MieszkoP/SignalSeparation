{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBwODPxEovdJ"
      },
      "source": [
        "##Generacja sygnałów **X** i zbioru zmiennych **y**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96AsyVfli1gf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras import datasets, layers, models, losses, Model\n",
        "from keras import backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZXC4idnfAqG",
        "outputId": "bbd359fe-dcaa-4261-a71a-636191d36681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SignalSeparation'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 50 (delta 12), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "if os.path.isdir('SignalSeparation'):\n",
        "    !rm -rf {'SignalSeparation'}\n",
        "!git clone https://github.com/MieszkoP/SignalSeparation.git\n",
        "from SignalSeparation import program as p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgBC4zAiFVyn"
      },
      "outputs": [],
      "source": [
        "size = 50000\n",
        "X_data = np.zeros([size,1000])\n",
        "Y_data = np.zeros([size,4])\n",
        "for i in range(size):\n",
        "  X_data[i,:], Y_data[i,0], Y_data[i,1], Y_data[i,2], Y_data[i,3] = p.GenerateSignal(np.random.uniform(0,100), 0.8, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMMAT1iIKgT2"
      },
      "outputs": [],
      "source": [
        "Y_new = p.Standardize(Y_data, 400, 500, 400, 600, 0.8, 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkLBNdWkrxNU"
      },
      "source": [
        "Zbiór **treningowy** i **walidacyjny**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcjHSmYWKtKd"
      },
      "outputs": [],
      "source": [
        "X = X_data\n",
        "\n",
        "size = Y_new.shape[0]\n",
        "\n",
        "Y_new_test = Y_new[:int(size*0.1),:]\n",
        "X_t = X[:int(size*0.1),:]\n",
        "\n",
        "Y_new = Y_new[int(size*0.1):,:]\n",
        "X = X[int(size*0.1):,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_a5Ljtfr9C6"
      },
      "source": [
        "## Utworzenie sieci neuronowej"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVHCpRbqr-sS"
      },
      "source": [
        "Funkcja wagi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCBh5aTcFVHk"
      },
      "outputs": [],
      "source": [
        "CostHeightProp = p.CostFunction(p.RatioOfUniforms, 0.8, 15, 1, p.HeightProportionTensorflow(p.DeStandarizeHeight(Y_new[:,0], 0.8, 15),p.DeStandarizeHeight(Y_new[:,1], 0.8, 15)))\n",
        "CostPlace2 = p.CostFunction(p.SumOfUniforms, 0.1, 0.9, 1, Y_new[:,3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUhP8vZFsvO1"
      },
      "source": [
        "Zmodyfikowana funkcja kosztu dla $h_{ratio}$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcAQHzHc8vu2"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error_new_height(y_true, y_pred, y_true1, y_true2):\n",
        "  y_pred = tf.convert_to_tensor(y_pred)\n",
        "  y_true = tf.cast(y_true, y_pred.dtype)\n",
        "\n",
        "  #Destandaryzacja do 0.8 - 15:\n",
        "\n",
        "  y_true1 = p.DeStandarizeHeight(y_true1, 0.8, 15)\n",
        "  y_true2 = p.DeStandarizeHeight(y_true2, 0.8, 15)\n",
        "  \n",
        "  return backend.mean(tf.math.squared_difference(y_pred, y_true)*CostHeightProp(p.HeightProportionTensorflow(y_true1, y_true2)), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB2_mk_LtF-L"
      },
      "outputs": [],
      "source": [
        "def height_dif(y_true, y_pred):\n",
        "  return mean_squared_error_new_height(p.HeightProportionTensorflow(y_true[:,1], y_true[:,0]),p.HeightProportionTensorflow(y_pred[:,1],y_pred[:,0]), y_true[:,0], y_true[:,1])*0.00255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjRi8_AUs86f"
      },
      "source": [
        "Zmodyfikowana funkcja kosztu dla $t_{m2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FelNFRfdZsR-"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error_new_position2(y_true, y_pred):\n",
        "  y_pred = tf.convert_to_tensor(y_pred)\n",
        "  y_true = tf.cast(y_true, y_pred.dtype)\n",
        "\n",
        "  #Destandaryzacja do 0.8 - 15:\n",
        "  \n",
        "  return backend.mean(tf.math.squared_difference(y_pred, y_true)*CostPlace2(y_true), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VxB58_OtN59"
      },
      "source": [
        "Funkcja kosztu dla $t_{\\Delta}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRDqEKvmWLF"
      },
      "outputs": [],
      "source": [
        "def freq_dif(y_true, y_pred):\n",
        "\n",
        "  true = p.PlacesChange(y_true[:,3], 0.1, 1.7)-y_true[:,2]\n",
        "  pred = p.PlacesChange(y_pred[:,3], 0.1, 1.7)-y_pred[:,2]\n",
        "  \n",
        "  return tf.keras.losses.mean_squared_error(true,pred)*0.224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JRUDLhntSoB"
      },
      "source": [
        "Funkcje kosztu dla $h_1$, $h_2$, $t_{m1}$ i dla $t_{m2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NamqD-nPLbHD"
      },
      "outputs": [],
      "source": [
        "def h1loss(y_true, y_pred):\n",
        "  return tf.keras.losses.mean_squared_error(y_true[:,0],y_pred[:,0])\n",
        "\n",
        "def h2loss(y_true, y_pred):\n",
        "  return tf.keras.losses.mean_squared_error(y_true[:,1],y_pred[:,1])\n",
        "\n",
        "def p1loss(y_true, y_pred):\n",
        "  return tf.keras.losses.mean_squared_error(y_true[:,2],y_pred[:,2])\n",
        "\n",
        "def p2loss(y_true, y_pred):\n",
        "  return mean_squared_error_new_position2(y_true[:,3],y_pred[:,3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sixLQuOvtiLI"
      },
      "source": [
        "Finalna funkcja kosztu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx5qWqSuLety"
      },
      "outputs": [],
      "source": [
        "def custom_loss_function(y_true, y_pred):\n",
        "  loss = height_dif(y_true, y_pred)+h1loss(y_true, y_pred)+h2loss(y_true, y_pred)+p1loss(y_true, y_pred)+p2loss(y_true, y_pred)+freq_dif(y_true, y_pred)\n",
        "  #print(y_true[0,:])\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xRcGrGPtlPO"
      },
      "source": [
        "Model trenowanej sieci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE8CTXrsggUA"
      },
      "outputs": [],
      "source": [
        "def TrainNetwork(seed, fork_point, optim, basic_lr, BatchNormalization):\n",
        "  tf.random.set_seed(seed)\n",
        "  \n",
        "  model = p.CreateNetwork(BatchNormalization, fork_point) #Ustal czy w architekturze sieci będzie normalizacja Batch na początku czy nie\n",
        "\n",
        "  opt = optim(learning_rate=basic_lr)\n",
        "  model.compile(loss=custom_loss_function,\n",
        "                optimizer=opt,\n",
        "                metrics = [h1loss, h2loss, p1loss, p2loss, height_dif, freq_dif],\n",
        "                run_eagerly=False)\n",
        "\n",
        "  earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min')\n",
        "  reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=4, verbose=1, min_lr=0.00001, min_delta=1e-4, mode='min')\n",
        "\n",
        "\n",
        "  mymodel=model.fit(\n",
        "      X,\n",
        "      Y_new,\n",
        "      batch_size=100,\n",
        "      epochs=300,\n",
        "      verbose=1,\n",
        "      callbacks=[reduce_lr_loss, earlyStopping],\n",
        "      validation_data = (X_t, Y_new_test)\n",
        "  )\n",
        "\n",
        "  print('Koniec uczenia')\n",
        "  return mymodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVK4cIT0t3N9"
      },
      "source": [
        "##Algorytm genetyczny"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvVCLRD0toW0"
      },
      "source": [
        "Definicja hiperparametrów i populacji algorytmu genetycznego"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfl0TsC3gEsr"
      },
      "outputs": [],
      "source": [
        "x = p.Populaton(5, 5) #Zdefiniowanie populacji i hiperparametrów \n",
        "x.define_feature(1, \"random state\", [1, 2, 3]) 1\n",
        "x.define_feature(2, \"network fork point\", [1,3,5,7])  2\n",
        "x.define_feature(3, \"optimizer\", [tf.keras.optimizers.Adam,tf.keras.optimizers.SGD, tf.keras.optimizers.Adagrad])  0\n",
        "x.define_feature(4, \"base learning rate\", [0.001, 0.01, 0.05, 0.1])  1\n",
        "x.define_feature(5, \"BatchNormalization\", [p.NoBatch, p.OnlyBeforeAct, p.OnlyAfterAct, p.AfterInput, p.AfterInputAndAfterAct, p.AfterInputAndBeforeAct])  2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4tVkfbltsIN"
      },
      "source": [
        "Wczytywanie wcześniej wytrenowanych hiperparametrów"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sRpXWEDNSeI"
      },
      "outputs": [],
      "source": [
        "x.Load('Stored.csv', 'Current.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbkeyy98twFZ"
      },
      "source": [
        "Trenowanie algorytmu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac2oIJe_gEob"
      },
      "outputs": [],
      "source": [
        "beg=0\n",
        "for itera in range(10):\n",
        "  if beg==1:\n",
        "    x.initialize()\n",
        "    beg=0\n",
        "  else:\n",
        "    x.IndBeg()\n",
        "\n",
        "  x.Check()\n",
        "  for i in range(5):\n",
        "    print(x.CheckIsMetricWrote())\n",
        "    if not x.CheckIsMetricWrote(): #Sprawdzenie czy nie ma już policzonej metryki\n",
        "      model = TrainNetwork(x.read(1), x.read(2), x.read(3), x.read(4), x.read(5))\n",
        "      if not np.isnan(model.history['loss'][-1]):\n",
        "        x.ReadMetric(-model.history['loss'][-1])\n",
        "      else:\n",
        "        x.ReadMetric(-100)\n",
        "    x.nextInd()\n",
        "\n",
        "  x.Store() #Zapisywanie w historii\n",
        "  print('Wynik:')\n",
        "  print(x.initialized)\n",
        "  x.Sorting() #Sortowanie\n",
        "  print('Wynik posortowany:')\n",
        "  print(x.initialized)\n",
        "  x.Selection() #Selekcja \n",
        "  print('Wynik wyselekcjonowany:')\n",
        "  print(x.initialized)\n",
        "  x.Crossing() #Krzyżowanie\n",
        "  print('Wynik krzyżowaniu:')\n",
        "  print(x.initialized)\n",
        "  x.Mutation(0.2) #Mutacja ze wsp. mutacji 20%\n",
        "  print('Wynik po mutacji:')\n",
        "  print(x.initialized)\n",
        "  x.Save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQdqwlvNJeHH"
      },
      "source": [
        "### Wizualizacja hiperparametrow algorytmu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcv2bFksJiuJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "table = np.loadtxt('/content/DataStored.txt')\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlYfOWXXL15e"
      },
      "outputs": [],
      "source": [
        "table2 = table[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKn6XVsOL-EO"
      },
      "outputs": [],
      "source": [
        "tabtab = table2.reshape(int(table2.shape[0]/5),5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vaXgznnKZxv"
      },
      "outputs": [],
      "source": [
        "plt.scatter(np.arange(1,tabtab.shape[0]+1),-tabtab[:,0])\n",
        "\n",
        "plt.scatter(np.arange(1,tabtab.shape[0]+1),-tabtab[:,1])\n",
        "\n",
        "plt.scatter(np.arange(1,tabtab.shape[0]+1),-tabtab[:,2])\n",
        "\n",
        "plt.scatter(np.arange(1,tabtab.shape[0]+1),-tabtab[:,3])\n",
        "\n",
        "plt.scatter(np.arange(1,tabtab.shape[0]+1),-tabtab[:,4])\n",
        "\n",
        "plt.ylim(0, 0.07)\n",
        "plt.ylabel('Wynik funkcji kosztu')\n",
        "plt.xlabel('Nr generacji')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D97t1ERTuGtx"
      },
      "source": [
        "##Wyniki pracy sieci (trenowanie bez SVR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Miq_LJwdvPhw"
      },
      "outputs": [],
      "source": [
        "model = TrainNetwork(2, 5, tf.keras.optimizers.Adam, 0.01, p.OnlyAfterAct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjxd3hReYPO4"
      },
      "outputs": [],
      "source": [
        "model.model.save_weights('wagi.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x0bEbUZYpGV"
      },
      "outputs": [],
      "source": [
        "model.model.load_weights('wagi.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1pDQYcgDDvY"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model.model, \"model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imQJ0A13DW62"
      },
      "outputs": [],
      "source": [
        "y_pred_train = model.model.predict(X)\n",
        "y_pred = model.model.predict(X_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK1CPaCTDil1"
      },
      "outputs": [],
      "source": [
        "Y_destand = p.DeStandarize(Y_new, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_test_destand = p.DeStandarize(Y_new_test, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_pred_test_destand = p.DeStandarize(y_pred, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_pred_destand = p.DeStandarize(y_pred_train, 400, 500, 400, 600, 0.8, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTmWmXDdBlQJ"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-GpXxrYBbxd"
      },
      "outputs": [],
      "source": [
        "Y_destand_l, Y_test_destand_l, Y_pred_test_destand_l, Y_pred_destand_l = p.ExcludeByDistance(400, 10, Y_destand, Y_test_destand, Y_pred_test_destand, Y_pred_destand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzy8v56yBcLa"
      },
      "outputs": [],
      "source": [
        "dane_pred0 = np.array(p.HeightProportionTensorflow(Y_pred_destand_l[:,0], Y_pred_destand_l[:,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zbiór treningowy"
      ],
      "metadata": {
        "id": "bl5pkdApPoTe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqtWKGWpBoPZ"
      },
      "outputs": [],
      "source": [
        "arr= np.array([])\n",
        "string = ['wysokość pierwszego sygnału', 'wysokość drugiego sygnału', 'środek pierwszego sygnału','środek drugiego sygnału']\n",
        "for i in range(4):\n",
        "  p.PredRealChart(string[i], Y_pred_destand_l[:,i], Y_destand_l[:,i])\n",
        "  print(p.R2(Y_pred_destand_l[:,i], Y_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.R2(Y_pred_destand_l[:,i], Y_destand_l[:,i]))\n",
        "  print(p.NMSE(Y_pred_destand_l[:,i], Y_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.NMSE(Y_pred_destand_l[:,i], Y_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.ZYSK(34, 34)) #Nalezy zmodyfikować w taki sposób by kalkulowalo ZYSK\n",
        "hr=6\n",
        "string = ['wysokość wyższ. sygnału / wysokość niższ. sygnału', 'środek pierwszego sygnału - środek drugiego sygnału', 'wys. wyższ. sygnału / wys. niższ. sygnału dla $h_{ratio}$>'+str(hr)]\n",
        "\n",
        "dane0 = np.array(p.HeightProportionTensorflow(Y_destand_l[:,1],Y_destand_l[:,0]))\n",
        "dane1 = Y_destand_l[:,3]-Y_destand_l[:,2]\n",
        "\n",
        "dane_pred0 = p.HeightProportionTensorflow(Y_pred_destand_l[:,0], Y_pred_destand_l[:,1])\n",
        "dane_pred1 = Y_pred_destand_l[:,3]-Y_pred_destand_l[:,2]\n",
        "\n",
        "p.PredRealChart(string[0], dane_pred0, dane0)\n",
        "print(p.R2(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.R2(dane_pred0, dane0))\n",
        "print(p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "p.PredRealChart(string[1], dane_pred1, dane1)\n",
        "print(p.R2(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.R2(dane_pred1, dane1))\n",
        "print(p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "newdane, newdane_pred = np.array([]), np.array([])\n",
        "for i in range(len(dane0)):\n",
        "  if dane0[i]>hr:\n",
        "    newdane = np.append(newdane, dane0[i])\n",
        "    newdane_pred = np.append(newdane_pred, dane_pred0[i])\n",
        "\n",
        "p.PredRealChart(string[2], newdane_pred, newdane)\n",
        "\n",
        "print(p.R2(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.R2(newdane_pred, newdane))\n",
        "print(p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zapisywanie wyników NMSE dla zmiennych"
      ],
      "metadata": {
        "id": "uG4oNXBFPzEU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaHdVC9xBsGR"
      },
      "outputs": [],
      "source": [
        "arr = arr.reshape(7,3)\n",
        "arr2 = arr.sum(axis=0)\n",
        "arr3 = np.vstack((arr, arr2))\n",
        "np.savetxt(\"wyniki_zkosztem.txt\", arr3, fmt='%.4f', delimiter=' & ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zbiór walidacyjny"
      ],
      "metadata": {
        "id": "IWhozQ4_Prgl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyqIkRVTn2nq"
      },
      "outputs": [],
      "source": [
        "arr= np.array([])\n",
        "string = ['wysokość pierwszego sygnału', 'wysokość drugiego sygnału', 'środek pierwszego sygnału','środek drugiego sygnału']\n",
        "for i in range(4):\n",
        "  p.PredRealChart(string[i], Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i])\n",
        "  print(p.R2(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.R2(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  print(p.NMSE(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.NMSE(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.ZYSK(34, 34))\n",
        "hr=6\n",
        "string = ['wysokość wyższ. sygnału / wysokość niższ. sygnału', 'środek pierwszego sygnału - środek drugiego sygnału', 'wys. wyższ. sygnału / wys. niższ. sygnału dla $h_{ratio}$>'+str(hr)]\n",
        "\n",
        "dane0 = np.array(p.HeightProportionTensorflow(Y_test_destand_l[:,1],Y_test_destand_l[:,0]))\n",
        "dane1 = Y_test_destand_l[:,3]-Y_test_destand_l[:,2]\n",
        "\n",
        "dane_pred0 = p.HeightProportionTensorflow(Y_pred_test_destand_l[:,0], Y_pred_test_destand_l[:,1])\n",
        "dane_pred1 = Y_pred_test_destand_l[:,3]-Y_pred_test_destand_l[:,2]\n",
        "\n",
        "p.PredRealChart(string[0], dane_pred0, dane0)\n",
        "print(p.R2(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.R2(dane_pred0, dane0))\n",
        "print(p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "p.PredRealChart(string[1], dane_pred1, dane1)\n",
        "print(p.R2(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.R2(dane_pred1, dane1))\n",
        "print(p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "newdane, newdane_pred = np.array([]), np.array([])\n",
        "for i in range(len(dane0)):\n",
        "  if dane0[i]>hr:\n",
        "    newdane = np.append(newdane, dane0[i])\n",
        "    newdane_pred = np.append(newdane_pred, dane_pred0[i])\n",
        "\n",
        "p.PredRealChart(string[2], newdane_pred, newdane)\n",
        "\n",
        "print(p.R2(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.R2(newdane_pred, newdane))\n",
        "print(p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC--o27GjuMf"
      },
      "source": [
        "## Optymalizacja parametrow $\\gamma$ i $C$.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pic5tKfbnM7q"
      },
      "outputs": [],
      "source": [
        "size = 8000\n",
        "X_data = np.zeros([size,1000])\n",
        "Y_data = np.zeros([size,4])\n",
        "for i in range(size):\n",
        "  X_data[i,:], Y_data[i,0], Y_data[i,1], Y_data[i,2], Y_data[i,3] = p.GenerateSignal(np.random.uniform(0,100), 0.8, 15)\n",
        "\n",
        "Y_new = p.Standardize(Y_data, 400, 500, 400, 600, 0.8, 15)\n",
        "\n",
        "X = X_data\n",
        "\n",
        "size = Y_new.shape[0]\n",
        "\n",
        "Y_new_test = Y_new[:int(size*0.5),:]\n",
        "X_t = X[:int(size*0.5),:]\n",
        "\n",
        "Y_new = Y_new[int(size*0.5):,:]\n",
        "X = X[int(size*0.5):,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjHiBQ33jtmr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.svm import SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHEXpI9TSME_"
      },
      "outputs": [],
      "source": [
        "model = p.CreateNetwork(p.OnlyAfterAct, 5)\n",
        "model.load_weights('wagi.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBLoG_z5kkYd"
      },
      "outputs": [],
      "source": [
        "hvector = Model(inputs=model.input,\n",
        "                                 outputs=model.layers[-5].output)\n",
        "pvector = Model(inputs=model.input,\n",
        "                                 outputs=model.layers[-4].output)\n",
        "\n",
        "Xp = pvector.predict(X)\n",
        "Xh = hvector.predict(X)\n",
        "\n",
        "Xp_t = pvector.predict(X_t)\n",
        "Xh_t = hvector.predict(X_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnAMP3Z8lpxE"
      },
      "outputs": [],
      "source": [
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.svm import SVR\n",
        "# y - cztery kolumny\n",
        "\n",
        "svr = SVR(epsilon=0.001,  kernel='rbf', C=10, gamma=1)\n",
        "mor2 = MultiOutputRegressor(svr)\n",
        "mor2.fit(Xh, Y_new[:, :2])\n",
        "pred_train1 = mor2.predict(Xh)\n",
        "pred_test1 = mor2.predict(Xh_t)\n",
        "\n",
        "mor2.fit(Xp, Y_new[:, 2:])\n",
        "pred_train2 = mor2.predict(Xp)\n",
        "pred_test2 = mor2.predict(Xp_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H3sS4OJFdgo"
      },
      "outputs": [],
      "source": [
        "y_pred_train = np.hstack((pred_train1, pred_train2))\n",
        "y_pred = np.hstack((pred_test1, pred_test2))\n",
        "Y_destand = p.DeStandarize(Y_new, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_test_destand = p.DeStandarize(Y_new_test, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_pred_test_destand = p.DeStandarize(y_pred, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_pred_destand = p.DeStandarize(y_pred_train, 400, 500, 400, 600, 0.8, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHna5y_Tl-bC"
      },
      "outputs": [],
      "source": [
        "arr0 = np.zeros((0,9))\n",
        "arr2 = np.zeros((0,9))\n",
        "for c in [1, 10, 100, 1000, 10000]:\n",
        "  for gamma in [0.001, 0.01, 0.1, 1, 5]:\n",
        "    arr0 = arr2\n",
        "    print(c, gamma)\n",
        "\n",
        "    \n",
        "\n",
        "    svr = SVR(epsilon=0.001,  kernel='rbf', C=c, gamma=gamma)\n",
        "    mor1 = MultiOutputRegressor(svr)\n",
        "    mor1.fit(Xh, Y_new[:, :2])\n",
        "    pred_train1 = mor1.predict(Xh)\n",
        "    pred_test1 = mor1.predict(Xh_t)\n",
        "\n",
        "\n",
        "    svr = SVR(epsilon=0.001,  kernel='rbf', C=c, gamma=gamma)\n",
        "    mor2 = MultiOutputRegressor(svr)\n",
        "    mor2.fit(Xp, Y_new[:, 2:])\n",
        "    pred_train2 = mor2.predict(Xp)\n",
        "    pred_test2 = mor2.predict(Xp_t)\n",
        "\n",
        "    y_pred_train = np.hstack((pred_train1, pred_train2))\n",
        "    y_pred = np.hstack((pred_test1, pred_test2))\n",
        "\n",
        "    Y_destand = p.DeStandarize(Y_new, 400, 500, 400, 600, 0.8, 15)\n",
        "    Y_test_destand = p.DeStandarize(Y_new_test, 400, 500, 400, 600, 0.8, 15)\n",
        "    Y_pred_test_destand = p.DeStandarize(y_pred, 400, 500, 400, 600, 0.8, 15)\n",
        "    Y_pred_destand = p.DeStandarize(y_pred_train, 400, 500, 400, 600, 0.8, 15)\n",
        "\n",
        "    Y_destand_l, Y_test_destand_l, Y_pred_test_destand_l, Y_pred_destand_l = p.ExcludeByDistance(400, 10, Y_destand, Y_test_destand, Y_pred_test_destand, Y_pred_destand)\n",
        "    dane_pred0 = np.array(p.HeightProportionTensorflow(Y_pred_destand_l[:,0], Y_pred_destand_l[:,1]))\n",
        "\n",
        "    arr = np.array([c, gamma])\n",
        "\n",
        "    \n",
        "    for i in range(4):\n",
        "      print(p.R2(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "      print(p.NMSE(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "\n",
        "      arr = np.hstack([arr, p.NMSE(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i])])\n",
        "\n",
        "    hr=6\n",
        "\n",
        "    dane0 = np.array(p.HeightProportionTensorflow(Y_test_destand_l[:,1],Y_test_destand_l[:,0]))\n",
        "    dane1 = Y_test_destand_l[:,3]-Y_test_destand_l[:,2]\n",
        "\n",
        "    dane_pred0 = p.HeightProportionTensorflow(Y_pred_test_destand_l[:,0], Y_pred_test_destand_l[:,1])\n",
        "    dane_pred1 = Y_pred_test_destand_l[:,3]-Y_pred_test_destand_l[:,2]\n",
        "\n",
        "    print(p.R2(dane_pred0, dane0))\n",
        "    print(p.NMSE(dane_pred0, dane0))\n",
        "    arr = np.hstack([arr, p.NMSE(dane_pred0, dane0)])\n",
        "\n",
        "    print(p.R2(dane_pred1, dane1))\n",
        "    print(p.NMSE(dane_pred1, dane1))\n",
        "    arr = np.hstack([arr, p.NMSE(dane_pred1, dane1)])\n",
        "\n",
        "    #arr = np.array([])\n",
        "\n",
        "    newdane, newdane_pred = np.array([]), np.array([])\n",
        "    for i in range(len(dane0)):\n",
        "      if dane0[i]>hr:\n",
        "        newdane = np.append(newdane, dane0[i])\n",
        "        newdane_pred = np.append(newdane_pred, dane_pred0[i])\n",
        "\n",
        "    print(p.R2(newdane_pred, newdane))\n",
        "    print(p.NMSE(newdane_pred, newdane))\n",
        "    arr = np.hstack([arr, p.NMSE(newdane_pred, newdane)])\n",
        "\n",
        "    #print(arr)\n",
        "    arr2 = np.vstack([arr0, arr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UNwwlsaL67iF"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(arr2, columns=['c', 'gamma', 'h1', 'h2', 't_m1', 't_m2', 'h_ratio', 't_delta','h_ratio if h_ratio>6'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw1jJ4GW7ezr"
      },
      "outputs": [],
      "source": [
        "h1 = df.pivot('c', 'gamma', 'h1')\n",
        "h2 = df.pivot('c', 'gamma', 'h2')\n",
        "\n",
        "p1 = df.pivot('c', 'gamma', 'p1')\n",
        "p2 = df.pivot('c', 'gamma', 'p2')\n",
        "\n",
        "h_ratio = df.pivot('c', 'gamma', 'h_ratio')\n",
        "pdelta = df.pivot('c', 'gamma', 'pdelta')\n",
        "h_ratio6 = df.pivot('c', 'gamma', 'h_ratio6')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcTiLHEu6VI9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns; sns.set_theme(style='white')\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm, Normalize\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.subplot(2,4,1)\n",
        "plt.title('NMSE dla $h_1$')\n",
        "sns.heatmap(h1, square=True, norm=LogNorm(), cmap='gray')\n",
        "\n",
        "plt.subplot(2,4,2)\n",
        "plt.title('NMSE dla $h_2$')\n",
        "sns.heatmap(h2, square=True, norm=LogNorm(), cmap='gray')\n",
        "\n",
        "plt.subplot(2,4,3)\n",
        "plt.title('NMSE dla $t_{m1}$')\n",
        "sns.heatmap(p1, square=True, norm=LogNorm(), cmap='gray')\n",
        "\n",
        "plt.subplot(2,4,4)\n",
        "plt.title('NMSE dla $t_{m2}$')\n",
        "sns.heatmap(p2, square=True, norm=LogNorm(), cmap='gray')\n",
        "\n",
        "plt.subplot(2,4,5)\n",
        "plt.title('NMSE dla $h_{ratio}$')\n",
        "sns.heatmap(h_ratio, square=True, norm=LogNorm(), cmap='gray')\n",
        "\n",
        "plt.subplot(2,4,6)\n",
        "plt.title('NMSE dla $t_{\\Delta}$')\n",
        "sns.heatmap(pdelta, square=True, norm=LogNorm(), cmap='gray')\n",
        "\n",
        "plt.subplot(2,4,7)\n",
        "plt.title('NMSE dla $h_{ratio}>6$')\n",
        "sns.heatmap(h_ratio6, square=True, norm=LogNorm(), cmap='gray')\n",
        "\n",
        "plt.subplot(2,4,7)\n",
        "plt.title('Suma NMSE dla wszystkich zmiennych$')\n",
        "sns.heatmap(h_ratio6, square=True, norm=LogNorm(), cmap='gray')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jxB4h1wOhGH"
      },
      "source": [
        "##Finalna Ocena"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FitECWsVPIO7"
      },
      "outputs": [],
      "source": [
        "size = 50000\n",
        "X_data = np.zeros([size,1000])\n",
        "Y_data = np.zeros([size,4])\n",
        "for i in range(size):\n",
        "  X_data[i,:], Y_data[i,0], Y_data[i,1], Y_data[i,2], Y_data[i,3] = p.GenerateSignal(np.random.uniform(0,100), 0.8, 15)\n",
        "\n",
        "Y_new = p.Standardize(Y_data, 400, 500, 400, 600, 0.8, 15)\n",
        "\n",
        "X = X_data\n",
        "\n",
        "size = Y_new.shape[0]\n",
        "\n",
        "Y_new_test = Y_new[:int(size*0.1),:]\n",
        "X_t = X[:int(size*0.1),:]\n",
        "\n",
        "Y_new = Y_new[int(size*0.1):,:]\n",
        "X = X[int(size*0.1):,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlAE251LOlXI"
      },
      "outputs": [],
      "source": [
        "model = p.CreateNetwork(p.OnlyAfterAct, 5)\n",
        "model.load_weights('wagi.h5')\n",
        "\n",
        "hvector = Model(inputs=model.input,\n",
        "                                 outputs=model.layers[-5].output)\n",
        "pvector = Model(inputs=model.input,\n",
        "                                 outputs=model.layers[-4].output)\n",
        "\n",
        "Xp = pvector.predict(X)\n",
        "Xh = hvector.predict(X)\n",
        "\n",
        "Xp_t = pvector.predict(X_t)\n",
        "Xh_t = hvector.predict(X_t)\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "svr = SVR(epsilon=0.001,  kernel='rbf', C=1, gamma=0.1)\n",
        "mor2 = MultiOutputRegressor(svr)\n",
        "mor2.fit(Xh, Y_new[:, :2])\n",
        "pred_train1 = mor2.predict(Xh)\n",
        "pred_test1 = mor2.predict(Xh_t)\n",
        "\n",
        "mor2.fit(Xp, Y_new[:, 2:])\n",
        "pred_train2 = mor2.predict(Xp)\n",
        "pred_test2 = mor2.predict(Xp_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFzaEj1FQP3x"
      },
      "outputs": [],
      "source": [
        "y_pred_train = np.hstack((pred_train1, pred_train2))\n",
        "y_pred = np.hstack((pred_test1, pred_test2))\n",
        "Y_destand = p.DeStandarize(Y_new, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_test_destand = p.DeStandarize(Y_new_test, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_pred_test_destand = p.DeStandarize(y_pred, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_pred_destand = p.DeStandarize(y_pred_train, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_destand_l, Y_test_destand_l, Y_pred_test_destand_l, Y_pred_destand_l = p.ExcludeByDistance(400, 10, Y_destand, Y_test_destand, Y_pred_test_destand, Y_pred_destand)\n",
        "dane_pred0 = np.array(p.HeightProportionTensorflow(Y_pred_destand_l[:,0], Y_pred_destand_l[:,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1aJISM4Q-ye"
      },
      "source": [
        "###Zbiór treningowy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCwI7k16QaGJ"
      },
      "outputs": [],
      "source": [
        "arr= np.array([])\n",
        "string = ['wysokość pierwszego sygnału', 'wysokość drugiego sygnału', 'środek pierwszego sygnału','środek drugiego sygnału']\n",
        "for i in range(4):\n",
        "  p.PredRealChart(string[i], Y_pred_destand_l[:,i], Y_destand_l[:,i])\n",
        "  print(p.R2(Y_pred_destand_l[:,i], Y_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.R2(Y_pred_destand_l[:,i], Y_destand_l[:,i]))\n",
        "  print(p.NMSE(Y_pred_destand_l[:,i], Y_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.NMSE(Y_pred_destand_l[:,i], Y_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.ZYSK(34, 34))\n",
        "hr=6\n",
        "string = ['wysokość wyższ. sygnału / wysokość niższ. sygnału', 'środek pierwszego sygnału - środek drugiego sygnału', 'wys. wyższ. sygnału / wys. niższ. sygnału dla $h_{ratio}$>'+str(hr)]\n",
        "\n",
        "dane0 = np.array(p.HeightProportionTensorflow(Y_destand_l[:,1],Y_destand_l[:,0]))\n",
        "dane1 = Y_destand_l[:,3]-Y_destand_l[:,2]\n",
        "\n",
        "dane_pred0 = p.HeightProportionTensorflow(Y_pred_destand_l[:,0], Y_pred_destand_l[:,1])\n",
        "dane_pred1 = Y_pred_destand_l[:,3]-Y_pred_destand_l[:,2]\n",
        "\n",
        "p.PredRealChart(string[0], dane_pred0, dane0)\n",
        "print(p.R2(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.R2(dane_pred0, dane0))\n",
        "print(p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "p.PredRealChart(string[1], dane_pred1, dane1)\n",
        "print(p.R2(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.R2(dane_pred1, dane1))\n",
        "print(p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "newdane, newdane_pred = np.array([]), np.array([])\n",
        "for i in range(len(dane0)):\n",
        "  if dane0[i]>hr:\n",
        "    newdane = np.append(newdane, dane0[i])\n",
        "    newdane_pred = np.append(newdane_pred, dane_pred0[i])\n",
        "\n",
        "p.PredRealChart(string[2], newdane_pred, newdane)\n",
        "\n",
        "print(p.R2(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.R2(newdane_pred, newdane))\n",
        "print(p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaVyjIZ3RA-x"
      },
      "source": [
        "###Zbiór testowy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPLOWZjgRCsC"
      },
      "outputs": [],
      "source": [
        "arr= np.array([])\n",
        "string = ['wysokość pierwszego sygnału', 'wysokość drugiego sygnału', 'środek pierwszego sygnału','środek drugiego sygnału']\n",
        "for i in range(4):\n",
        "  p.PredRealChart(string[i], Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i])\n",
        "  print(p.R2(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.R2(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  print(p.NMSE(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.NMSE(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.ZYSK(34, 34))\n",
        "hr=6\n",
        "string = ['wysokość wyższ. sygnału / wysokość niższ. sygnału', 'środek pierwszego sygnału - środek drugiego sygnału', 'wys. wyższ. sygnału / wys. niższ. sygnału dla $h_{ratio}$>'+str(hr)]\n",
        "\n",
        "dane0 = np.array(p.HeightProportionTensorflow(Y_test_destand_l[:,1],Y_test_destand_l[:,0]))\n",
        "dane1 = Y_test_destand_l[:,3]-Y_test_destand_l[:,2]\n",
        "\n",
        "dane_pred0 = p.HeightProportionTensorflow(Y_pred_test_destand_l[:,0], Y_pred_test_destand_l[:,1])\n",
        "dane_pred1 = Y_pred_test_destand_l[:,3]-Y_pred_test_destand_l[:,2]\n",
        "\n",
        "p.PredRealChart(string[0], dane_pred0, dane0)\n",
        "print(p.R2(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.R2(dane_pred0, dane0))\n",
        "print(p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "p.PredRealChart(string[1], dane_pred1, dane1)\n",
        "print(p.R2(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.R2(dane_pred1, dane1))\n",
        "print(p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "newdane, newdane_pred = np.array([]), np.array([])\n",
        "for i in range(len(dane0)):\n",
        "  if dane0[i]>hr:\n",
        "    newdane = np.append(newdane, dane0[i])\n",
        "    newdane_pred = np.append(newdane_pred, dane_pred0[i])\n",
        "\n",
        "p.PredRealChart(string[2], newdane_pred, newdane)\n",
        "\n",
        "print(p.R2(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.R2(newdane_pred, newdane))\n",
        "print(p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPSAI8s3ZqsC"
      },
      "outputs": [],
      "source": [
        "arr = arr.reshape(7,3)\n",
        "arr20 = arr.sum(axis=0)\n",
        "arr9 = np.vstack((arr, arr20))\n",
        "np.savetxt(\"wyniki5.txt\", arr9, fmt='%.4f', delimiter=' & ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LMJMQBPbySO"
      },
      "source": [
        "##Trenowanie bez modyfikacji funkcji kosztu (dla porównania)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4OwRky4cZS_"
      },
      "source": [
        "Funkcje kosztu dla $h_1$, $h_2$, $t_{m1}$ i dla $t_{m2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5RS7dskcO1i"
      },
      "outputs": [],
      "source": [
        "def h1loss(y_true, y_pred):\n",
        "  return tf.keras.losses.mean_squared_error(y_true[:,0],y_pred[:,0])\n",
        "\n",
        "def h2loss(y_true, y_pred):\n",
        "  return tf.keras.losses.mean_squared_error(y_true[:,1],y_pred[:,1])\n",
        "\n",
        "def p1loss(y_true, y_pred):\n",
        "  return tf.keras.losses.mean_squared_error(y_true[:,2],y_pred[:,2])\n",
        "\n",
        "def p2loss(y_true, y_pred):\n",
        "  return tf.keras.losses.mean_squared_error(y_true[:,3],y_pred[:,3])\n",
        "\n",
        "def freq_dif(y_true, y_pred):\n",
        "\n",
        "  true = p.PlacesChange(y_true[:,3], 0.1, 1.7)-y_true[:,2]\n",
        "  pred = p.PlacesChange(y_pred[:,3], 0.1, 1.7)-y_pred[:,2]\n",
        "  return tf.keras.losses.mean_squared_error(true,pred)*0.224\n",
        "\n",
        "def height_dif(y_true, y_pred):\n",
        "  return tf.keras.losses.mean_squared_error(p.HeightProportionTensorflow(y_true[:,1], y_true[:,0]),p.HeightProportionTensorflow(y_pred[:,1],y_pred[:,0]))*0.00255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX3cjCyycvuQ"
      },
      "outputs": [],
      "source": [
        "def custom_loss_function(y_true, y_pred):\n",
        "  loss = height_dif(y_true, y_pred)+h1loss(y_true, y_pred)+h2loss(y_true, y_pred)+p1loss(y_true, y_pred)+p2loss(y_true, y_pred)+freq_dif(y_true, y_pred)\n",
        "  #print(y_true[0,:])\n",
        "  return loss\n",
        "\n",
        "def TrainNetwork(seed, fork_point, optim, basic_lr, BatchNormalization):\n",
        "  tf.random.set_seed(seed)\n",
        "  \n",
        "  model = p.CreateNetwork(BatchNormalization, fork_point) #Ustal czy w architekturze sieci będzie normalizacja Batch na początku czy nie\n",
        "\n",
        "  opt = optim(learning_rate=basic_lr)\n",
        "  model.compile(loss=custom_loss_function,\n",
        "                optimizer=opt,\n",
        "                metrics = [h1loss, h2loss, p1loss, p2loss, height_dif, freq_dif],\n",
        "                run_eagerly=False)\n",
        "\n",
        "  earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min')\n",
        "  reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=4, verbose=1, min_lr=0.00001, min_delta=1e-4, mode='min')\n",
        "\n",
        "\n",
        "  mymodel=model.fit(\n",
        "      X,\n",
        "      Y_new,\n",
        "      batch_size=100,\n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      callbacks=[reduce_lr_loss, earlyStopping],\n",
        "      validation_data = (X_t, Y_new_test)\n",
        "  )\n",
        "\n",
        "  print('Koniec uczenia')\n",
        "  return mymodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KdCQCrysdRt-"
      },
      "outputs": [],
      "source": [
        "model = TrainNetwork(2, 5, tf.keras.optimizers.Adam, 0.01, p.OnlyAfterAct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9ytaFCRkhd6M"
      },
      "outputs": [],
      "source": [
        "y_pred_train = model.model.predict(X)\n",
        "y_pred = model.model.predict(X_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ISu0Lo-ShokE"
      },
      "outputs": [],
      "source": [
        "Y_destand = p.DeStandarize(Y_new, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_test_destand = p.DeStandarize(Y_new_test, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_pred_test_destand = p.DeStandarize(y_pred, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_pred_destand = p.DeStandarize(y_pred_train, 400, 500, 400, 600, 0.8, 15)\n",
        "Y_destand_l, Y_test_destand_l, Y_pred_test_destand_l, Y_pred_destand_l = p.ExcludeByDistance(400, 10, Y_destand, Y_test_destand, Y_pred_test_destand, Y_pred_destand)\n",
        "dane_pred0 = np.array(p.HeightProportionTensorflow(Y_pred_destand_l[:,0], Y_pred_destand_l[:,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ELu2R9YehxFS"
      },
      "outputs": [],
      "source": [
        "arr= np.array([])\n",
        "string = ['wysokość pierwszego sygnału', 'wysokość drugiego sygnału', 'środek pierwszego sygnału','środek drugiego sygnału']\n",
        "for i in range(4):\n",
        "  p.PredRealChart(string[i], Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i])\n",
        "  print(p.R2(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.R2(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  print(p.NMSE(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.NMSE(Y_pred_test_destand_l[:,i], Y_test_destand_l[:,i]))\n",
        "  arr = np.append(arr, p.ZYSK(34, 34))\n",
        "hr=6\n",
        "string = ['wysokość wyższ. sygnału / wysokość niższ. sygnału', 'środek pierwszego sygnału - środek drugiego sygnału', 'wys. wyższ. sygnału / wys. niższ. sygnału dla $h_{ratio}$>'+str(hr)]\n",
        "\n",
        "dane0 = np.array(p.HeightProportionTensorflow(Y_test_destand_l[:,1],Y_test_destand_l[:,0]))\n",
        "dane1 = Y_test_destand_l[:,3]-Y_test_destand_l[:,2]\n",
        "\n",
        "dane_pred0 = p.HeightProportionTensorflow(Y_pred_test_destand_l[:,0], Y_pred_test_destand_l[:,1])\n",
        "dane_pred1 = Y_pred_test_destand_l[:,3]-Y_pred_test_destand_l[:,2]\n",
        "\n",
        "p.PredRealChart(string[0], dane_pred0, dane0)\n",
        "print(p.R2(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.R2(dane_pred0, dane0))\n",
        "print(p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.NMSE(dane_pred0, dane0))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "p.PredRealChart(string[1], dane_pred1, dane1)\n",
        "print(p.R2(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.R2(dane_pred1, dane1))\n",
        "print(p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.NMSE(dane_pred1, dane1))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))\n",
        "\n",
        "newdane, newdane_pred = np.array([]), np.array([])\n",
        "for i in range(len(dane0)):\n",
        "  if dane0[i]>hr:\n",
        "    newdane = np.append(newdane, dane0[i])\n",
        "    newdane_pred = np.append(newdane_pred, dane_pred0[i])\n",
        "\n",
        "p.PredRealChart(string[2], newdane_pred, newdane)\n",
        "\n",
        "print(p.R2(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.R2(newdane_pred, newdane))\n",
        "print(p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.NMSE(newdane_pred, newdane))\n",
        "arr = np.append(arr, p.ZYSK(34, 34))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qytCvOlJh32y"
      },
      "outputs": [],
      "source": [
        "arr = arr.reshape(7,3)\n",
        "arr20 = arr.sum(axis=0)\n",
        "arr9 = np.vstack((arr, arr20))\n",
        "np.savetxt(\"wyniki5.txt\", arr9, fmt='%.4f', delimiter=' & ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Walidacja krzyżowa 10-krotna"
      ],
      "metadata": {
        "id": "S9QBiRLowmIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainNetwork(seed, fork_point, optim, basic_lr, BatchNormalization, X, Y_new):\n",
        "  tf.random.set_seed(seed)\n",
        "  \n",
        "  model = p.CreateNetwork(BatchNormalization, fork_point) #Ustal czy w architekturze sieci będzie normalizacja Batch na początku czy nie\n",
        "\n",
        "  opt = optim(learning_rate=basic_lr)\n",
        "  model.compile(loss=custom_loss_function,\n",
        "                optimizer=opt,\n",
        "                metrics = [h1loss, h2loss, p1loss, p2loss, height_dif, freq_dif],\n",
        "                run_eagerly=False)\n",
        "\n",
        "  earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min')\n",
        "  reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=4, verbose=1, min_lr=0.00001, min_delta=1e-4, mode='min')\n",
        "\n",
        "\n",
        "  mymodel=model.fit(\n",
        "      X,\n",
        "      Y_new,\n",
        "      batch_size=100,\n",
        "      epochs=300,\n",
        "      verbose=1,\n",
        "      callbacks=[reduce_lr_loss, earlyStopping],\n",
        "      validation_data = (X_t, Y_new_test)\n",
        "  )\n",
        "\n",
        "  print('Koniec uczenia')\n",
        "  return mymodel"
      ],
      "metadata": {
        "id": "iCPrtIg728b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 50000\n",
        "X_data = np.zeros([size,1000])\n",
        "Y_data = np.zeros([size,4])\n",
        "for i in range(size):\n",
        "  X_data[i,:], Y_data[i,0], Y_data[i,1], Y_data[i,2], Y_data[i,3] = p.GenerateSignal(np.random.uniform(0,100), 0.8, 15)\n",
        "\n",
        "Y_new_all = p.Standardize(Y_data, 400, 500, 400, 600, 0.8, 15)\n",
        "\n",
        "size = Y_new_all.shape[0]"
      ],
      "metadata": {
        "id": "xgkL99VbwoRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for it in np.arange(1,11):\n",
        "  Y_new_test = Y_new_all[int(size*0.1*(it-1)):int(size*0.1*it),:]\n",
        "  X_t = X_data[int(size*0.1*(it-1)):int(size*0.1*it),:]\n",
        "\n",
        "  Y_new = np.vstack((Y_new_all[:int(size*0.1*(it-1)),:], Y_new_all[int(size*0.1*it):,:]))\n",
        "  X = np.vstack((X_data[:int(size*0.1*(it-1)),:], X_data[int(size*0.1*it):,:]))\n",
        "\n",
        "  print('\\n treningowy: ')\n",
        "  print(Y_new[-1,:])\n",
        "  print(Y_new.shape)\n",
        "\n",
        "  print('\\n testowy: ')\n",
        "  print(Y_new_test.shape)\n",
        "  print(Y_new_test[-1,:])\n",
        "\n",
        "  model = TrainNetwork(2, 5, tf.keras.optimizers.Adam, 0.01, p.OnlyAfterAct, X, Y_new)\n"
      ],
      "metadata": {
        "id": "fgQ-2BluxLGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Symulacja identyfikacji próbki"
      ],
      "metadata": {
        "id": "y-NO6kb6fIcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=20\n",
        "\n",
        "def GenerateSignal(height_ratio, df, bottom_border, top_border): #Wygeneruj sygnał o ustalonym stosunku wysokości pików (pik2/pik1) i o ustalonej odległości df. Rozmiar pików losowy (ale w taki spoób aby stosunek wysokości sie zgadzał), miejsce tez losowe (od 400 do 500)\n",
        "  \n",
        "  if height_ratio<0:\n",
        "    height1 = np.random.uniform(0.8,15/(abs(height_ratio)+1))\n",
        "    height2 = height1*(abs(height_ratio)+1)\n",
        "  else:\n",
        "    height1 = np.random.uniform(0.8*(abs(height_ratio)+1),15)\n",
        "    height2 = height1/(abs(height_ratio)+1)\n",
        "\n",
        "\n",
        "  x = np.arange(0,1000,1)\n",
        "  place1 = np.random.uniform(400,500)\n",
        "  place2 = place1+df\n",
        "  y_f1 = scipy.stats.norm(place1, 100/2.355)\n",
        "  y1 = y_f1.pdf(x)*100/2.355*(2*3.14)**(1/2)*height1\n",
        "  y_f2 = scipy.stats.norm(place2, 100/2.355)\n",
        "  y2 = y_f2.pdf(x)*100/2.355*(2*3.14)**(1/2)*height2\n",
        "  y = y1+y2\n",
        "  y = np.random.normal(loc=0, scale=0.005, size=1000)+y\n",
        "  return y, height1, height2, place1, place2"
      ],
      "metadata": {
        "id": "qFZswGpSfIDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predicting(model_n, mor2a, mor2b, signal):\n",
        "  inp = np.zeros((1,1000))\n",
        "  inp[0,:] = signal\n",
        "  \n",
        "  \n",
        "\n",
        "  Xp = pvector.predict(inp)\n",
        "  Xh = hvector.predict(inp)\n",
        "\n",
        "  out1 = mor2a.predict(Xh)\n",
        "  out2 = mor2b.predict(Xp)\n",
        "  \n",
        "  out = np.hstack((out1, out2))\n",
        "  out = p.DeStandarize(out, 400, 500, 400, 600, 0.8, 15)\n",
        "  return out"
      ],
      "metadata": {
        "id": "O-81fY19fO0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
        "\n",
        "for j in range(0,10):\n",
        "  print(\"Stosunek wysokości = \"+str(j+1))\n",
        "  plt.figure(figsize=[20,10])\n",
        "  for i in range(0,10):\n",
        "    #print(np.log10(j))\n",
        "    data = GenerateSignal(j,40, 0.8, 15)\n",
        "    predicted = predicting(model, mor2a, mor2b, data[0])[0,:]\n",
        "\n",
        "    plt.subplot(2,4,1)\n",
        "    plt.scatter(data[3], predicted[2])\n",
        "    plt.xlabel('Prawdziwe położenie 1 piku')\n",
        "    plt.ylabel('Przewidywane położenie 1 piku')\n",
        "    plt.plot([400, 500], [400, 500], '--')\n",
        "\n",
        "    plt.subplot(2,4,2)\n",
        "    plt.scatter(data[4], predicted[3])\n",
        "    plt.xlabel('Prawdziwe położenie 2 piku')\n",
        "    plt.ylabel('Przewidywane położenie 2 piku')\n",
        "    plt.plot([400, 600], [400, 600], '--')\n",
        "\n",
        "    plt.subplot(2,4,3)\n",
        "    plt.scatter(data[1], predicted[0])\n",
        "    plt.xlabel('Prawdziwa wysokość 1 piku')\n",
        "    plt.ylabel('Przewidywana wysokość 1 piku')\n",
        "    plt.plot([0.8, 15], [0.8, 15], '--')\n",
        "\n",
        "    plt.subplot(2,4,4)\n",
        "    plt.scatter(data[2], predicted[1])\n",
        "    plt.xlabel('Prawdziwa wysokość 2 piku')\n",
        "    plt.ylabel('Przewidywana wysokość 2 piku')\n",
        "    plt.plot([0.8, 15], [0.8, 15], '--')\n",
        "\n",
        "    plt.subplot(2,4,7)\n",
        "    plt.scatter(data[1]/data[2], predicted[0]/predicted[1])\n",
        "    plt.xlabel('Prawdziwy stosunek wysokości pików')\n",
        "    plt.ylabel('Przewidywany stosunek wysokości pików')\n",
        "    plt.ylim(0,10)\n",
        "    plt.xlim(0,10)\n",
        "    plt.plot([0, 10], [0, 10], '--')\n",
        "\n",
        "    plt.subplot(2,4,8)\n",
        "    plt.scatter(data[4]-data[3], predicted[3]-predicted[2])\n",
        "    plt.xlabel('Prawdziwa różnica pików')\n",
        "    plt.ylabel('Przewidywana różnica pików')\n",
        "    plt.xlim(30,50)\n",
        "    plt.ylim(0,100)\n",
        "    #plt.plot([0.8, 15], [0.8, 15], '--')\n",
        "\n",
        "    #plt.\n",
        "    plt.subplot(2,2,3)\n",
        "    plt.plot(data[0])\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Sygnał')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "MvUmMl4_fSLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulations_height = np.ones((20, 8, 10))\n",
        "simulations_dif = np.ones((20, 8, 10))"
      ],
      "metadata": {
        "id": "EUzfKYBwfUMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
        "\n",
        "for j in range(0,20):\n",
        "  print(\"Stosunek wysokości = \"+str(j+1))\n",
        "  plt.figure(figsize=[20,10])\n",
        "  for i in range(0,10):\n",
        "    #print(np.log10(j))\n",
        "    data = GenerateSignal(j,40, 0.8, 15)\n",
        "    \n",
        "    predicted = predicting(model, mor2a, mor2b, data[0])[0,:]\n",
        "    predicted[3]-predicted[2]\n",
        "    predicted[0]/predicted[1]"
      ],
      "metadata": {
        "id": "_0NZ5vw1fXCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for height in range(0,10):\n",
        "  for t_delta in range(0,8):\n",
        "    for n in range(0,20):\n",
        "      data = GenerateSignal(height,10*(t_delta+1), 0.8, 15)\n",
        "      predicted = predicting(model, mor2a, mor2b, data[0])[0,:]\n",
        "      simulations_height[n, t_delta, height] = predicted[0]/predicted[1]\n",
        "      simulations_dif[n, t_delta, height] = predicted[3]-predicted[2]"
      ],
      "metadata": {
        "id": "G9GP04VdfeIY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ProjektInzynierski6.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}